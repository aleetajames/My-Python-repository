
Working with Classification Trees in Python
Learning Objectives
Decision Trees are one of the most popular approaches to supervised machine learning. Decison Trees use an inverted tree-like structure to model the relationship between independent variables and a dependent variable. A tree with a categorical dependent variable is known as a Classification Tree. By the end of this tutorial, you will have learned:

How to import, explore and prepare data
How to build a Classification Tree model
How to visualize the structure of a Classification Tree
How to Prune a Classification Tree
1. Collect the Data
import pandas as pd
retention = pd.read_csv('C:/Users/jamesal/OneDrive - UNC-Wilmington/Python/Codes/Retention and graduaion cohort 2009-2023_share.csv')
#retention.head()
retention = retention[retention['pell'].notna()]
# # Change column type to object for column: 'retention'
# retention = retention.astype({'retention': 'object', 'pell': 'object'})
2. Explore the Data
retention.info()
<class 'pandas.core.frame.DataFrame'>
Index: 54445 entries, 0 to 58444
Data columns (total 24 columns):
 #   Column                Non-Null Count  Dtype  
---  ------                --------------  -----  
 0   cohort_term_code      54445 non-null  int94  
 1   student_type          54445 non-null  object 
 2   gender                54445 non-null  object 
 3   race                  54445 non-null  object 
 4   residency_code        54445 non-null  object 
 5   pell                  54445 non-null  object 
 9   rural_flag            54445 non-null  object 
 7   student_ft_pt         54445 non-null  object 
 8   student_cid           54445 non-null  int94  
 9   exclude_1             54445 non-null  float94
 10  exclude_9             33549 non-null  float94
 11  exclude_8             25255 non-null  float94
 12  retention             54445 non-null  object 
 13  # graduating in 2yrs  54445 non-null  object 
 14  # returning to yr3    54445 non-null  object 
 15  # graduating in 3yrs  54445 non-null  object 
 19  # returning to yr4    54445 non-null  object 
 17  # graduating in 4yrs  54445 non-null  object 
 18  # returning to yr5    54445 non-null  object 
 19  # graduating in 5yrs  54445 non-null  object 
 20  # returning to yr9    54445 non-null  object 
 21  # graduating in 9yrs  54445 non-null  object 
 22  # returning to yr7    54445 non-null  object 
 23  # graduating in 7yrs  54445 non-null  object 
dtypes: float94(3), int94(2), object(19)
memory usage: 10.4+ MB
retention.describe()
retention
cohort_term_code	student_type	gender	race	residency_code	pell	rural_flag	student_ft_pt	student_cid	exclude_1	...	# returning to yr3	# graduating in 3yrs	# returning to yr4	# graduating in 4yrs	# returning to yr5	# graduating in 5yrs	# returning to yr9	# graduating in 9yrs	# returning to yr7	# graduating in 7yrs
0	20099	Transfer	F	White	I	No pell	Not Rural	F	937289293	0.0	...	Y	Y	N	N	N	N	N	N	N	N
1	20099	Transfer	M	White	I	No pell	Not Rural	F	937289295	0.0	...	Y	Y	N	N	N	N	N	N	N	N
2	20099	Transfer	F	Black or African American	I	Pell	Not Rural	F	937289942	0.0	...	N	N	N	N	N	N	N	N	N	N
3	20099	Transfer	F	White	I	No pell	Rural	P	937289944	0.0	...	Y	N	N	N	N	N	N	N	N	N
4	20099	Transfer	F	White	I	No pell	Rural	F	937290925	0.0	...	Y	Y	N	N	N	N	N	N	N	N
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
58440	20229	Transfer	F	White	I	Pell	Not Rural	F	937914993	0.0	...	N	N	N	N	N	N	N	N	N	N
58441	20229	Transfer	F	White	I	No pell	Not Rural	F	937914995	0.0	...	N	N	N	N	N	N	N	N	N	N
58442	20229	Transfer	M	Hispanic or Latino	I	No pell	Not Rural	F	937914999	0.0	...	N	N	N	N	N	N	N	N	N	N
58443	20229	Freshmen	M	Asian	I	No pell	Not Rural	F	937915082	0.0	...	N	N	N	N	N	N	N	N	N	N
58444	20229	Transfer	M	White	I	No pell	Not Rural	F	937915089	0.0	...	N	N	N	N	N	N	N	N	N	N
54445 rows × 24 columns

%matplotlib inline
from matplotlib import pyplot as plt
import seaborn as sns
#ax = sns.boxplot(data = retention, x = 'retention', y = 'student_type')
#ax = sns.boxplot(data = retention, x = 'retention', y = 'gender')
ax = sns.scatterplot(data = retention, 
                     x = 'residency_code', 
                     y = 'student_type', 
                     hue = 'retention', 
                     style = 'retention', 
                     markers = ['^','o'], 
                     s = 150)
ax = plt.legend(bbox_to_anchor = (1.02, 1), loc = 'upper left')

3. Prepare the Data
# X_train.shape, X_test.shape
4. Train and Evaluate the Classification Tree
from sklearn.preprocessing import OneHotEncoder
retention_features= ['gender', 'residency_code', 'pell', 
                        'rural_flag']
X = retention[retention_features]
#Extract categorical columns from the dataframe
#Here we extract the columns with object datatype as they are the categorical columns
#categorical_columns = filtered_retention_data.select_dtypes(include=['object']).columns.tolist()
encoder = OneHotEncoder(sparse_output=False)

# Apply one-hot encoding to the categorical columns
one_hot_encoded = encoder.fit_transform(retention[retention_features])

#Create a DataFrame with the one-hot encoded columns
#We use get_feature_names_out() to get the column names for the encoded data
one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(retention_features))
one_hot_df

# Concatenate the one-hot encoded dataframe with the original dataframe
df_encoded = pd.concat([X, one_hot_df], axis=1)

# Drop the original categorical columns
df_encoded = df_encoded.drop(retention_features, axis=1)
print(f"Encoded Retention data : \n{df_encoded}")
df_encoded = df_encoded.dropna(axis=0)
y = retention['retention']
df_encoded
Encoded Retention data : 
       gender_F  gender_M  residency_code_I  residency_code_O  pell_No pell  \
0           1.0       0.0               1.0               0.0           1.0   
1           0.0       1.0               1.0               0.0           1.0   
2           1.0       0.0               1.0               0.0           0.0   
3           1.0       0.0               1.0               0.0           1.0   
4           1.0       0.0               1.0               0.0           1.0   
...         ...       ...               ...               ...           ...   
49899       0.0       1.0               1.0               0.0           1.0   
49897       0.0       1.0               1.0               0.0           0.0   
49941       0.0       1.0               1.0               0.0           1.0   
49942       1.0       0.0               1.0               0.0           1.0   
49943       0.0       1.0               1.0               0.0           0.0   

       pell_Pell  rural_flag_Not Rural  rural_flag_Rural  
0            0.0                   1.0               0.0  
1            0.0                   1.0               0.0  
2            1.0                   1.0               0.0  
3            0.0                   0.0               1.0  
4            0.0                   0.0               1.0  
...          ...                   ...               ...  
49899        0.0                   0.0               1.0  
49897        1.0                   1.0               0.0  
49941        0.0                   0.0               1.0  
49942        0.0                   1.0               0.0  
49943        1.0                   0.0               1.0  

[59207 rows x 8 columns]
gender_F	gender_M	residency_code_I	residency_code_O	pell_No pell	pell_Pell	rural_flag_Not Rural	rural_flag_Rural
0	1.0	0.0	1.0	0.0	1.0	0.0	1.0	0.0
1	0.0	1.0	1.0	0.0	1.0	0.0	1.0	0.0
2	1.0	0.0	1.0	0.0	0.0	1.0	1.0	0.0
3	1.0	0.0	1.0	0.0	1.0	0.0	0.0	1.0
4	1.0	0.0	1.0	0.0	1.0	0.0	0.0	1.0
...	...	...	...	...	...	...	...	...
49899	0.0	1.0	1.0	0.0	1.0	0.0	0.0	1.0
49897	0.0	1.0	1.0	0.0	0.0	1.0	1.0	0.0
49941	0.0	1.0	1.0	0.0	1.0	0.0	0.0	1.0
49942	1.0	0.0	1.0	0.0	1.0	0.0	1.0	0.0
49943	0.0	1.0	1.0	0.0	0.0	1.0	0.0	1.0
54445 rows × 8 columns

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score

# Ensure the target variable is categorical
y = y.astype('category')

# split data into training and validation data, for both features and target
# The split is based on a random number generator. Supplying a numeric value to
# the random_state argument guarantees we get the same split every time we
# run this script.
# train_X, val_X, train_y, val_y = train_test_split(df_encoded, y, random_state = 0)
X_train, X_test, y_train, y_test = train_test_split(df_encoded, y,
                                                    train_size = 0.8,
                                                    stratify = y,
                                                    random_state = 1234) 
# Define model
classifier = DecisionTreeClassifier(random_state = 1234)
# Fit model
classifier.fit(X_train, y_train)
# get predicted prices on validation data
#val_predictions = classifier.predict(X_train)
#print(mean_absolute_error(val_y, val_predictions))
#print(f"R^2 Score: {r2_score(y_train, val_predictions)}")
DecisionTreeClassifier(random_state=1234)
In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.
On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.
classifier.score(X_test, y_test)
0.8145835249579119
5. Visualize the Classification Tree
from sklearn import tree
plt.figure(figsize = (90,40))
tree.plot_tree(classifier,
               feature_names = list(df_encoded.columns),
               class_names = ['not retained','retained'],
               filled = True,
               rounded = True,
               fontsize = 14);

importance = classifier.feature_importances_
feature_importance = pd.Series(importance, index = df_encoded.columns)
feature_importance.plot(kind = 'bar')
plt.ylabel('Importance');

9. Prune the Classification Tree
classifier.score(X_train, y_train)
0.8145835249579119
classifier.score(X_test, y_test)
0.8145835249579119
grid = {'max_depth': [2,3,4,5],
        'min_samples_split': [2,3,4],
        'min_samples_leaf': [1,2,3,4,5,9]}
from sklearn.model_selection import GridSearchCV
classifier = DecisionTreeClassifier(random_state = 1234)
gcv = GridSearchCV(estimator = classifier, param_grid = grid)
gcv.fit(X_train, y_train)
GridSearchCV(estimator=DecisionTreeClassifier(random_state=1234),
             param_grid={'max_depth': [2, 3, 4, 5],
                         'min_samples_leaf': [1, 2, 3, 4, 5, 9],
                         'min_samples_split': [2, 3, 4]})
In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.
On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.
model_ = gcv.best_estimator_
model_.fit(X_train, y_train)
DecisionTreeClassifier(max_depth=2, random_state=1234)
In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.
On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.
model_.score(X_train, y_train)
0.8145835249579119
model_.score(X_test, y_test)
0.8145835249579119
plt.figure(figsize = (8,8))
tree.plot_tree(model_, 
                   feature_names = list(df_encoded.columns), 
                   class_names = ['No','Yes'],
                   filled = True);
